<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS180 Project 4</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 20px;
            line-height: 1.6;
        }

        .side-by-side {
            display: flex;
            justify-content: center;
            gap: 20px;
            margin-top: 20px;
        }

        .image-container {
            display: flex;
            flex-direction: column;
            align-items: center;
            margin: 20px 0;
        }

        img {
            max-width: 100%;
            height: auto;
            border: 2px solid #ddd;
            border-radius: 8px;
            display: block; /* Ensures images are treated as block elements */
            margin: 0 auto; /* Center the images */
        }

        figcaption {
            font-size: 0.9em;
            color: #555;
            text-align: center;
            margin-top: 5px;
        }

        p {
            text-align: left; /* Left justify paragraphs */
            max-width: 800px; /* Adjust this value to change width */
            margin: 10px auto; /* Center the paragraph with some margin */
        }
    </style>
</head>
<body>

    <h1>CS180 Project 5A</h1>

    <p> This is the website for CS180 Project 5, where we learned a lot more about diffusion models and image generation. I thought the coolest part of the project was building the time and class conditioned UNet and finding that class conditioned UNet does a really good job of generating images similar to the numbers in the MNIST dataset.</p>

    <h2> Downloading Precomputed Text Embeddings </h2>

    <div class="image-container">
        <div class="side-by-side">
            <figure>
                <img src="20_inf.png">
                <figcaption>20 inference steps.</figcaption>
            </figure>
            <figure>
                <img src="5_inf.png">
                <figcaption> 5 inference steps. </figcaption>
            </figure>
        </div>
    </div>

    <p> To start, we wanted to learn a little more about diffusion models. We started by looking at some precomputed text embeddings and sample images that were generated. The three captions given were "an oil painting of a snowy mountain village", "a man wearing a hat", "a rocket ship". The first three have 20 inference steps and the next three have 5 inference steps. For the 20 inference steps, all the images do seem to basically exactly match the text prompts and the quality seems pretty good, except that the colors are pretty off; like the rocket ship seems more of a caricature rather than an actual depiction of a rocket ship. For the five inference steps the quality got a lot worse, especially again with the rocket ship which is now kind of broken apart into pieces with some strange things on top of it. I think this just supports the hypothesis that at least at the beginning, more inference steps can improve quality of outputs. The random seed I am using is 2988.</p>

    <h2> Implementing the Forward Process </h2>

    <div class="image-container">
        <figure>
            <img src="noise_levels.png">
            <figcaption> The original image, noisy image at t = 250, noisy image at t = 500, noisy image at t = 750 </figcaption>
        </figure>
    </div>

    <p> For the forward process, I exactly followed the two equations given where you compute a noisy image x_t by sampling from a Gaussian that is constructed according to alpha-bar-t and random noise epsilon, specifically equation A.2. from the project spec. This resulted in the following test image at noise levels [250, 500, 750]; as you can clearly see as the noise level increases, the image gets more and more noisy since we inject more random noise. </p>

    <h2> Classical Denoising </h2>

    <div class="image-container">
        <figure>
            <img src="denoised.png">
            <figcaption> The noisy image followed by the Gaussian blurred noisy image with a 5x5 kernel at t = 250, t = 500, t = 750. The results are not great, and especially at t = 750 not much of the original image really appears to be reconstructed. </figcaption>
        </figure>
    </div>

    <p> Next I tried to use the denoising tactics that we have used in previous projects, which is applying a Gaussian blur to the image with torchvision.transforms.functional.gaussian_blur to try to remove the noise. For this I used a 5x5 kernel. Above are the noisy images and corresponding Gaussian-denoised versions. This did not do so well, so it felt necessary to try another method of denoising. </p>

    <h2> One-Step Denoising </h2>

    <div class="image-container">
        <figure>
            <img src="one-step.png">
            <figcaption> Top is the original image. Then, in sequence, noisy images and one-step denoising. Note that the outputs at later steps have less noise around them, but don't necessarily look that much like the original image, for example the shapes and colors of the building is a little off. </figcaption>
        </figure>
    </div>

    <p> Now, we do one-step denoising. For this piece I used a UNet conditioned on the amount of Gaussian noise to first estimate the noise, then remove that noise to get an estimate of the original image. Note that when removing the noise, I had to reverse the process by which the noise was applied, dealing with the alpha-bar-t scalars and doing the "backwards" of what we do to add the noise by taking im_noisy - sqrt(1 - alpha-bar-t) * noise_est / sqrt(alpha-bar-t). This then got images that looked slightly better than Gaussian blur, but still not amazing, as described in the caption above. Thus, we turn to iterative denoising.</p>

    <h2> Iterative Denoising </h2>

    <div class="image-container">
        <figure>
            <img src="iterative.png">
            <figcaption> The image through iterative denoising, showing the result every 5 timesteps and at the end the very final result of the iterative denoising. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="compare_iterative.png">
            <figcaption> The image through that we did the iterative denoising on, but with one-step denoising (on the top) and Gaussian blur (on the bottom). We see that iterative denoising clearly does a lot better. </figcaption>
        </figure>
    </div>

    <p> Now, we do iterative denoising following equations 6 and 7 of the DDPM paper. The key idea here is that we keep improving the image and denoising over multiple time steps, each time getting a noise estimate and removing it, so we are able to better estimate the total noise over time sort of thinking of it like the sum of these noise estimates. This iterative denoising result is displayed above, as you see over each 5 timesteps (since we displayed a result every 5 timesteps), the image gets better and better as more and more noise is removed. We can contrast these results to the second set of images above, which is the same image with one-step denoising and Gaussian blur, to see iterative denoising does better. </p>

    <h2> Diffusion Model Sampling </h2>

    <div class="image-container">
        <figure>
            <img src="diffusion_generated.png">
            <figcaption> The five generated images using iterative_denoise on pure noise. </figcaption>
        </figure>
    </div>

    <p> Next we do Diffusion Model Sampling. Here, we use the iterative_denoise function with i_start = 0 and passing in completely random noise in order to generate an image from scratch, denoising a completely random image (pure noise) to then generate an image that wasn't already there. Above are our five generated images using the iterative_denoise function to denoise pure noise. <p>

    <h2> Classifier Free Guidance </h2>

    <div class="image-container">
        <figure>
            <img src="cfg.png">
            <figcaption> The five images generated using CFG with gamma = 7. Notice that these are much higher quality than the previous images generated. </figcaption>
        </figure>
    </div>

    <p> Next we want to improve upon these generated images a little so we do Classifier-Free Guidance (CFG). This computes a conditional and unconditional noise estimate and then lets the new noise estimate be equal to the unconditional estimate + gamma * (conditional noise estimate - unconditional noise estimate). For the unconditional noise estimate, we just use an empty prompt embedding whereas for the conditional we use the "a high quality photo" embedding. The gamma parameter controls how much of the unconditional/conditional estimate you use, and utilizing a gamma of 7, we notice that when gamma > 1 you actually get a lot of higher quality images. You can see these images above. </p>

    <h2> Image to Image Translation </h2> 
    <div class="image-container">
        <figure>
            <img src="im2im_1.png">
            <figcaption> Image to image translation with the campanile image using starting index [1, 3, 5, 7, 10, 20]. As you can see as the starting index increases, the less "edits" we do and the closer to the actual campanile image we get. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="im2im_2.png">
            <figcaption> Image to image translation with an image of a cat off Wikipedia using starting index [1, 3, 5, 7, 10, 20]. This process does slightly worse, but the final images do look closer and closer to a cat. The bottom image is the original image, rest are at each time step. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="im2im_3.png">
            <figcaption> Image to image translation with an image of a random couple off Pinterest because that's the only pinimg link I could find. Using starting index [1, 3, 5, 7, 10, 20]. This does a lot better than the cat. The bottom image is the original image, rest are at each time step. </figcaption>
        </figure>
    </div>

    <p> Next we do image to image translation. We take an original image, add some noise, and then use the SDEdit algorithm to get back a new image using the text prompt "a high quality photo". This should return an image similar to the test image, as long as the noise is low enough. We do this by running forward to get a noisy test image, then run iterative_denoise_cfg with different starting indexes to see the "edits" to the image, the less the starting index, the closer to the original image that the image generated is. </p>

    <h2> Editing Hand Drawn and Web Images </h2> 

    <div class="image-container">
        <figure>
            <img src="avocado.png">
            <figcaption> The avocado image at each timestep, same process as the above images. Again, we see that as we increase the number of steps the image looks less and less like the original avocado (and as i-start increases we get closer to the original image). </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="envelope.png">
            <figcaption> I hand drew an image that looked kind of like an envelope and got these results. The last image looks a lot like what I drew except with an additional smiley face, which is cute. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="heart.png">
            <figcaption> I hand drew a heart, and the last image looks a lot like it, but I thought it was kind of funny that second to last looks more like a lightbulb. </figcaption>
        </figure>
    </div>

    <p> Now, we do the same thing with one image from the web (an avocado pinterest image) and two hand drawn images. These are the results. This shows that "edits" and image to image translation work not just on real life images but also on hand drawn and web images, which is really cool.</p>

    <h2> Inpainting </h2> 

    <div class="image-container">
        <figure>
            <img src="setup.png">
            <figcaption> The setup for the campanile inpainting. See the picture, mask, and portion of the image. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="inpainted.png">
            <figcaption> The inpainted campanile, see how the top comes from a different image. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="setup_cat.png">
            <figcaption> The setup for the cat inpainting. See the picture, mask, and portion of the image. I set the mask to be the region of the cat's head. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="cat_inpainted.png">
            <figcaption> The cat inpainted, with a different head there. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="couple_setup.png">
            <figcaption> The setup for the couple inpainting. See the picture, mask, and portion of the image. I set the mask to be the man of the couple. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="couple_inpainted.png">
            <figcaption> The new inpainted, the girl looks like she is leaning against a wall instead of a man, which is cool. </figcaption>
        </figure>
    </div>

    <p> Next, I did inpainting. Here, we leave everything except for the mask part as the original image, and then do image generation in just the mask part. So x_t = mx_t + (1-m)*forward(x, t). This resulted in the above results, which are pretty cool to kind of combine a generated image onto part of the original image. </p>

    <h2> Text conditional Image to Image Translation </h2>

    <div class="image-container">
        <figure>
            <img src="textconditioned.png">
            <figcaption> The same as image to image translation, but with the prompt of "a rocket ship" using the campanile image. You see that it looks more kind of like a merge between a rocket ship and the campanile as it moves forward in timesteps. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="couplerocket.png">
            <figcaption> The same prompt of "a rocket ship" but with the pinterest image of a couple. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="siam.png">
            <figcaption> The same prompt of "a rocket ship" but with the cat image from wikipedia. </figcaption>
        </figure>
    </div>

    <p> Next, I did the same thing as above but with a text prompt, which conditions the image generation based on the text prompt as well. This just means you take the previous image generation prompt of "a high quality photo" and just give a text prompt of what you want the edits to look more like. The resulting images are above.</p>

    <h2> Visual Anagrams </h2>

    <div class="image-container">
        <figure>
            <img src="mancampfire.png">
            <figcaption> An example of a visual anagram with the top prompt as "an oil painting of an old man" and the bottom as "an oil painting of people around a campfire". </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="rocketpencil.png">
            <figcaption> An example of a visual anagram with the top prompt as "a rocket ship" and the bottom as "a pencil". This one turned out pretty good I think. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="doghat.png">
            <figcaption> An example of "a photo of a dog" on top and the bottom as "a man wearing a hat". This one is pretty cool since you can see how the dog face kind of goes through the hat. Probably my favorite of the three. </figcaption>
        </figure>
    </div>

    <p> After all of this, we then can make visual anagrams, which allow us to create images that are generated as optical illusions. The method behind this is that first, we denoise an image of noise with a text prompt of something you want the top to look like. Then, you flip the image and denoise with another prompt of something you want the bottom to look like. Then, you average the two noise estimates and do the diffusion denoising with the average of the two noise estimates. This then makes it so that if you look at the image from the top it looks like something, and from the bottom something else -- based on the text prompts generated. This was super cool, so above are a couple examples of that working in action.</p>

    <h2> Hybrid Images </h2>

    <div class="image-container">
        <figure>
            <img src="skullwaterfall.png">
            <figcaption> A skull from close up and a waterfall from far away. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="doghat2.png">
            <figcaption> A man from close up and a dog from far away. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="doghat2.png">
            <figcaption> A man from close up and a dog from far away. </figcaption>
        </figure>
    </div>

    <div class="image-container">
        <figure>
            <img src="rocketpencil2.png">
            <figcaption> A rocket from close up and a pencil from far away. </figcaption>
        </figure>
    </div>

    <p> Finally, we implement hybrid images like in Project 2. For these images, we first apply UNet to the image with two different text prompts. Then, we take the low pass of the noise estimate for the first and the high pass of the noise estimate for the second, and that creates our new noise estimate that we use for the diffusion. This should retain an image that looks like the low passed thing from close up and the high passed thing from far away. The logic here is the same as the logic behind low/high frequencies in project 2. You can see this in the above images. </p>
    <h1>CS180 Project 5B</h1>

    <p> Next, we move to project 5b, which is implementing an actual UNet diffusion model. </p>

</body>
</html>